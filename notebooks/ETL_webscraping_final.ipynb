{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dc836fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T00:27:24.112013Z",
     "start_time": "2021-09-09T00:27:24.033861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# # Data Collection\n",
    "# def data_collection(url, headers):\n",
    "#     # Request\n",
    "#     page = requests.get(url, headers=headers)\n",
    "#     # Beautiful soup object\n",
    "#     soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#     # =============================== Product Data ===============================\n",
    "#     products = soup.find('ul', class_='products-listing small')\n",
    "#     # List class\n",
    "#     products_list = soup.find_all('article', class_='hm-product-item')\n",
    "#     # product_id\n",
    "#     product_id = [p.get('data-articlecode') for p in products_list]\n",
    "#     # product_category\n",
    "#     product_category = [p.get('data-category') for p in products_list]\n",
    "#     products_listb = products.find_all('a', class_='link')\n",
    "#     # product_name\n",
    "#     product_name = [p.get_text() for p in products_listb]\n",
    "#     # List price\n",
    "#     products_listc = products.find_all('span', class_='price regular')\n",
    "#     # product_price\n",
    "#     product_price = [p.get_text() for p in products_listc]\n",
    "#     # Unifque\n",
    "#     data = pd.DataFrame([product_id, product_category, product_name, product_price]).T\n",
    "#     # Name columns\n",
    "#     data.columns = ['product_id', 'product_category', 'product_name', 'product_price']\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# # Data Collect by Product\n",
    "# def data_collection_by_product(data, headers):\n",
    "#     # Empty dataframe\n",
    "#     df_compositions = pd.DataFrame()\n",
    "#     # unique columns for all products\n",
    "#     aux = []\n",
    "#     df_pattern = pd.DataFrame(columns=['Art. No.', 'Composition', 'Fit', 'Size', 'Product safety'])\n",
    "#     for i in range(len(data)):\n",
    "#         # buscar  a url\n",
    "#         url = 'https://www2.hm.com/en_us/productpage.' + data.loc[i, 'product_id'] + '.html'\n",
    "#         logger.debug('Produto: %s', url)\n",
    "#         page = requests.get(url, headers=headers)\n",
    "#         # Beautiful soup object\n",
    "#         soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#         # ==================== product_color ======================\n",
    "#         product_list = soup.find_all('a', class_='filter-option miniature active') + soup.find_all('a',\n",
    "#                                                                                                    class_='filter-option miniature')\n",
    "#         # loop for\n",
    "#         color_name = [p.get('data-color') for p in product_list]\n",
    "#         # product_id\n",
    "#         product_id = [p.get('data-articlecode') for p in product_list]\n",
    "#         # cria o dataframe\n",
    "#         df_color = pd.DataFrame([product_id, color_name]).T\n",
    "#         df_color.columns = ['product_id', 'color_name']\n",
    "#         # tadas as cores\n",
    "#         for j in range(len(df_color)):\n",
    "#             # buscar  a url\n",
    "#             url = 'https://www2.hm.com/en_us/productpage.' + df_color.loc[j, 'product_id'] + '.html'\n",
    "#             logger.debug('Color: %s', url)\n",
    "#             page = requests.get(url, headers=headers)\n",
    "#             # Beautiful soup object\n",
    "#             soup = BeautifulSoup(page.text, 'html.parser')\n",
    "#             # ==================== product_name ====================\n",
    "#             product_name = soup.find_all('h1', class_='primary product-item-headline')\n",
    "#             product_name = product_name[0].get_text()\n",
    "#             # ==================== produc_price ====================\n",
    "#             product_price = soup.find_all('div', class_='primary-row product-item-price')\n",
    "#             product_price = re.findall(r'\\d+\\.?\\d+', product_price[0].get_text())[0]\n",
    "#             # ==================== composition  ======================\n",
    "#             product_composition_list = soup.find_all('div', class_='pdp-description-list-item')\n",
    "#             # loop for\n",
    "#             product_composition = [list(filter(None, p.get_text().split('\\n'))) for p in product_composition_list]\n",
    "#             # criar o dataframe e localizar a primeira linha\n",
    "#             df_composition = pd.DataFrame(product_composition).T\n",
    "#             df_composition.columns = df_composition.iloc[0]\n",
    "#             df_composition = df_composition.iloc[1:].fillna(method='ffill')\n",
    "#             # Remove pocket lining, shell and lining   <<<===========================================================\n",
    "#             df_composition['Composition'] = df_composition['Composition'].replace('Pocket lining: ', '', regex=True)\n",
    "#             df_composition['Composition'] = df_composition['Composition'].replace('Pocket: ', '', regex=True)\n",
    "#             df_composition['Composition'] = df_composition['Composition'].replace('Lining: ', '', regex=True)\n",
    "#             df_composition['Composition'] = df_composition['Composition'].replace('Shell: ', '', regex=True)\n",
    "#             # concatenar as colunas do df_composition com df_pattern\n",
    "#             df_composition = pd.concat([df_pattern, df_composition], axis=0)\n",
    "#             # raname columns\n",
    "#             df_composition.columns = ['product_id', 'composition', 'fit', 'size', 'product_safety']\n",
    "#             df_composition['product_name'] = product_name\n",
    "#             df_composition['product_pric'] = product_price\n",
    "#             # alguma nova coluna diferente guardar no aux\n",
    "#             aux = aux + df_composition.columns.tolist()\n",
    "#             # marge\n",
    "#             df_composition = pd.merge(df_composition, df_color, how='left', on='product_id')\n",
    "#             # concatenar final\n",
    "#             df_compositions = pd.concat([df_compositions, df_composition], axis=0)\n",
    "#     # Join showroom data + style\n",
    "#     df_compositions['style_id'] = df_compositions['product_id'].apply(lambda x: x[:-3])\n",
    "#     df_compositions['color_id'] = df_compositions['product_id'].apply(lambda x: x[-3:])\n",
    "#     # scrapy datetime\n",
    "#     df_compositions['scrapy_datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "#     return df_compositions\n",
    "\n",
    "\n",
    "# # Data Cleanig\n",
    "# def data_cleaning(data_product):\n",
    "#     # product_category\n",
    "#     df_data = data_product.dropna(subset=['product_id'])\n",
    "#     # product_name\n",
    "#     df_data['product_name'] = df_data['product_name'].str.replace('\\n', '')\n",
    "#     df_data['product_name'] = df_data['product_name'].str.replace('\\t', '')\n",
    "#     df_data['product_name'] = df_data['product_name'].str.replace('  ', '')\n",
    "#     df_data['product_name'] = df_data['product_name'].str.replace(' ', '_').str.lower()\n",
    "#     # product_price\n",
    "#     df_data['product_pric'] = df_data['product_pric'].astype(float)\n",
    "#     #     color_name\n",
    "#     df_data['color_name'] = df_data['color_name'].str.replace(' ', '_').str.lower()\n",
    "#     #     Fit\n",
    "#     df_data['fit'] = df_data['fit'].apply(lambda x: x.replace(' ', '_').lower())\n",
    "#     #     Size\n",
    "#     df_data['size_number'] = df_data['size'].apply(lambda x: re.search('\\d{3}', x).group(0) if pd.notnull(x) else x)\n",
    "#     #     Size Model\n",
    "#     df_data['size_model'] = df_data['size'].str.extract('(\\d+/\\d+)')\n",
    "#     #     DropSize\n",
    "#     df_data = df_data.drop(columns=['size'], axis=1)\n",
    "#     #     break composition\n",
    "#     df1 = df_data['composition'].str.split(',', expand=True).reset_index(drop=True)\n",
    "#     # # =======================    Composition    =======================\n",
    "#     # Cotton | Polyester | Elasterell-P | Elastane\n",
    "#     df_ref = pd.DataFrame(index=np.arange(len(df_data)), columns=['cotton', 'polyester', 'elastane', 'elasterell'])\n",
    "#     # # ===========================================================================================\n",
    "#     # ---------------->> Cotton\n",
    "#     df_cotton_0 = df1.loc[df1[0].str.contains('Cotton', na=True), 0]\n",
    "#     df_cotton_0.name = 'cotton'\n",
    "#     df_cotton_1 = df1.loc[df1[1].str.contains('Cotton', na=True), 1]\n",
    "#     df_cotton_1.name = 'cotton'\n",
    "#     # combine cotton\n",
    "#     df_cotton = df_cotton_0.combine_first(df_cotton_1)\n",
    "#     # concatenar\n",
    "#     df_ref = pd.concat([df_ref, df_cotton], axis=1)\n",
    "#     df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "#     # ---------------->> Polyester\n",
    "#     df_polyester_0 = df1.loc[df1[0].str.contains('Polyester', na=True), 0]\n",
    "#     df_polyester_0.name = 'polyester'\n",
    "#     df_polyester_1 = df1.loc[df1[1].str.contains('Polyester', na=True), 1]\n",
    "#     df_polyester_1.name = 'polyester'\n",
    "#     # combine\n",
    "#     df_polyester = df_polyester_0.combine_first(df_polyester_1)\n",
    "#     # concatenar\n",
    "#     df_ref = pd.concat([df_ref, df_polyester], axis=1)\n",
    "#     df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "#     # ---------------->> Elastane\n",
    "#     df_Elastane_1 = df1.loc[df1[1].str.contains('Elastane', na=True), 1]\n",
    "#     df_Elastane_1.name = 'elastane'\n",
    "#     df_Elastane_2 = df1.loc[df1[2].str.contains('Elastane', na=True), 2]\n",
    "#     df_Elastane_2.name = 'elastane'\n",
    "#     df_Elastane_3 = df1.loc[df1[3].str.contains('Elastane', na=True), 3]\n",
    "#     df_Elastane_3.name = 'elastane'\n",
    "#     # combine\n",
    "#     df_Elastane_c = df_Elastane_1.combine_first(df_Elastane_2)\n",
    "#     df_elastane = df_Elastane_c.combine_first(df_Elastane_3)\n",
    "#     # concatenar\n",
    "#     df_ref = pd.concat([df_ref, df_elastane], axis=1)\n",
    "#     df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "#     # ---------------->> Elasterell-P\n",
    "#     df_Elasterell = df1.loc[df1[1].str.contains('Elasterell-P', na=True), 1]\n",
    "#     df_Elasterell.name = 'elasterell'\n",
    "#     # concatenar\n",
    "#     df_ref = pd.concat([df_ref, df_Elasterell], axis=1)\n",
    "#     df_ref = df_ref.iloc[:, ~df_ref.columns.duplicated(keep='last')]\n",
    "#     # join of combine with product_id\n",
    "#     df_aux = pd.concat([df_data['product_id'].reset_index(drop=True), df_ref], axis=1)\n",
    "#     # deixar as porcentagens nas composições\n",
    "#     df_aux['cotton'] = df_aux['cotton'].apply(lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "#     df_aux['polyester'] = df_aux['polyester'].apply(\n",
    "#         lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "#     df_aux['elasterell'] = df_aux['elasterell'].apply(\n",
    "#         lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "#     df_aux['elastane'] = df_aux['elastane'].apply(\n",
    "#         lambda x: int(re.search('\\d+', x).group(0)) / 100 if pd.notnull(x) else x)\n",
    "#     # # ===========================================================================================\n",
    "#     # # final join\n",
    "#     df_aux = df_aux.groupby('product_id').max().reset_index().fillna(0)\n",
    "#     # merge\n",
    "#     df_data = pd.merge(df_data, df_aux, on='product_id', how='left')\n",
    "#     # # ===========================================================================================\n",
    "#     # deixar so as colunas\n",
    "#     df_data = df_data.drop(columns=['composition'], axis=1)\n",
    "#     # drop duplicate\n",
    "#     df_data = df_data.drop_duplicates()\n",
    "#     return df_data\n",
    "\n",
    "\n",
    "# # Data Insert\n",
    "# def data_insert(df_data):\n",
    "#     data_insert = df_data[[\n",
    "#         'product_id',\n",
    "#         'style_id',\n",
    "#         'color_id',\n",
    "#         'product_name',\n",
    "#         'color_name',\n",
    "#         'fit',\n",
    "#         'product_pric',\n",
    "#         'size_number',\n",
    "#         'size_model',\n",
    "#         'cotton',\n",
    "#         'polyester',\n",
    "#         'elastane',\n",
    "#         'elasterell',\n",
    "#         'scrapy_datetime'\n",
    "#     ]]\n",
    "#     # create database conection\n",
    "#     conn = create_engine('sqlite:///database_hm.sqlite', echo=False)\n",
    "#     # data insert\n",
    "#     data_insert.to_sql('vitrine', con=conn, if_exists='append', index=False)\n",
    "#     return None\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     # logging\n",
    "#     path = '/home/borges/'\n",
    "#     if not os.path.exists(path + 'Logs'):\n",
    "#         os.makedirs(path + 'Logs')\n",
    "#     logging.basicConfig(\n",
    "#         filename=path + 'Logs/webscraping_hm.log',\n",
    "#         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "#         datefmt='%Y-%m-%d %H:%M:%S',\n",
    "#         level=logging.DEBUG\n",
    "#     )\n",
    "#     logger = logging.getLogger('webscraping_hm')\n",
    "\n",
    "#     # parameters and constants\n",
    "\n",
    "#     headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}\n",
    "#     # Url\n",
    "#     url = 'https://www2.hm.com/en_us/men/products/jeans.html'\n",
    "\n",
    "#     # data collection\n",
    "#     data = data_collection(url, headers)\n",
    "#     logger.info('data collect done')\n",
    "#     # data collection by product\n",
    "#     data_product = data_collection_by_product(data, headers)\n",
    "#     logger.info('data collection by prodduct done')\n",
    "#     # data cleaning\n",
    "#     data_product_cleaned = data_cleaning(data_product)\n",
    "#     logger.info('data product cleaned done')\n",
    "#     # data insertion\n",
    "#     data_insert(data_product_cleaned)\n",
    "#     logger.info('data insertion done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63084ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-09T00:26:51.429938Z",
     "start_time": "2021-09-09T00:26:41.541320Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe1373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
